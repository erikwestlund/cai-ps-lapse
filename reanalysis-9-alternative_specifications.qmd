---
title: "Reanalysis Step 7: Alternative Propensity Score Specifications"
subtitle: "Comprehensive comparison of PS methods with multiple imputation"
author: "Erik Westlund"
date: "`r Sys.Date()`"
format: 
  html:
    toc: true
    toc-depth: 3
    fig-width: 10
    fig-height: 8
---

## Setup

```{r setup}
#| include: false
source("dependencies.R")
source("functions.R")  # Load formula definitions and helper functions
setup_analysis(seed = 2025)

library(MatchIt)
library(WeightIt)
library(twang)
library(cobalt)
library(mice)
library(survey)
library(ggplot2)
library(gridExtra)
library(parallel)
library(doParallel)
library(forestplot)

# Initialize logging
log_file <- init_log("alternative_ps_methods")

# Check for required files
required_files <- c(
  "imputed_datasets.rds",
  "variable_lists.rds"
)

missing_files <- c()
for (file in required_files) {
  if (!file.exists(file.path(reanalysis_data_dir, file))) {
    missing_files <- c(missing_files, file)
  }
}

if (length(missing_files) > 0) {
  stop(paste("Required files not found. Please run previous steps first:\n",
             "- Step 1 (Data Preparation) creates: variable_lists.rds\n",
             "- Step 2 (Multiple Imputation) creates: imputed_datasets.rds\n",
             "Missing files:", paste(missing_files, collapse = ", ")))
}

# Load imputed datasets
imputed_datasets <- readRDS(file.path(reanalysis_data_dir, "imputed_datasets.rds"))
n_imputations <- length(imputed_datasets)

# Load variable lists
variable_lists <- readRDS(file.path(reanalysis_data_dir, "variable_lists.rds"))
ps_model_vars <- variable_lists$ps_model_vars

# Create PS formula
ps_formula <- as.formula(paste("ever_lapse_binary ~", 
                               paste(ps_model_vars, collapse = " + ")))

# ANALYSIS MODE
analysis_mode <- "final"  # Should match imputation mode
max_imputations_to_process <- ifelse(analysis_mode == "test", 5, n_imputations)

# Caching configuration
use_cache <- TRUE
cache_dir <- file.path(reanalysis_data_dir, "ps_alternative_methods")
if (!dir.exists(cache_dir)) {
  dir.create(cache_dir, recursive = TRUE)
  log_message(paste0("Created cache directory: ", cache_dir))
}

# Parallel processing configuration
max_cores_for_ps <- 2  # Limit cores for memory management

log_message(paste0("Analysis mode: ", analysis_mode))
log_message(paste0("Processing ", max_imputations_to_process, " of ", n_imputations, " imputations"))
log_message(paste0("Caching: ", ifelse(use_cache, "ENABLED", "DISABLED")))
```

## Define Propensity Score Methods

```{r define-methods}
# Define all PS methods to compare (excluding twang_gbm which we'll load from reanalysis-4)
ps_methods <- list(
  # MatchIt Methods - Nearest Neighbor with replacement
  list(name = "nearest_glm", package = "matchit", method = "nearest", 
       distance = "glm", estimand = "ATT", replace = TRUE),
  
  list(name = "nearest_gam", package = "matchit", method = "nearest", 
       distance = "gam", estimand = "ATT", replace = TRUE),
  
  list(name = "nearest_gbm", package = "matchit", method = "nearest", 
       distance = "gbm", estimand = "ATT", replace = TRUE,
       distance.options = list(n.trees = 1000, interaction.depth = 3)),
  
  list(name = "nearest_lasso", package = "matchit", method = "nearest", 
       distance = "lasso", estimand = "ATT", replace = TRUE),
  
  list(name = "nearest_rpart", package = "matchit", method = "nearest", 
       distance = "rpart", estimand = "ATT", replace = TRUE),
  
  list(name = "nearest_mahalanobis", package = "matchit", method = "nearest", 
       distance = "mahalanobis", estimand = "ATT", replace = TRUE),
  
  # MatchIt - Subclassification  
  list(name = "subclass_glm", package = "matchit", method = "subclass", 
       distance = "glm", estimand = "ATT", subclass = 5),
  
  # WeightIt Methods - IPTW
  list(name = "cbps", package = "weightit", method = "cbps", 
       estimand = "ATT", over = FALSE),
  
  list(name = "entropy", package = "weightit", method = "ebal", 
       estimand = "ATT"),
  
  list(name = "bart", package = "weightit", method = "bart", 
       estimand = "ATT")
)

# Note: twang_gbm results will be loaded from reanalysis-4

log_message(paste0("Defined ", length(ps_methods), " PS methods to compare"))
```

## Propensity Score Estimation Functions

```{r ps-functions}
# Function to process a single PS method for one imputation
process_ps_method <- function(imputation_id, data, method_params, ps_formula, cache_dir, use_cache = TRUE) {
  method_name <- method_params$name
  cache_file <- file.path(cache_dir, paste0(method_name, "_imp", imputation_id, ".rds"))
  
  # Check cache
  if (use_cache && file.exists(cache_file)) {
    cat(paste0("  Loading cached results for ", method_name, " (imp ", imputation_id, ")\n"))
    return(readRDS(cache_file))
  }
  
  cat(paste0("  Estimating ", method_name, " for imputation ", imputation_id, "\n"))
  
  result <- tryCatch({
    if (method_params$package == "matchit") {
      # MatchIt methods
      args <- list(
        formula = ps_formula,
        data = data,
        method = method_params$method,
        estimand = method_params$estimand
      )
      
      # Add method-specific parameters
      if (!is.null(method_params$distance)) args$distance <- method_params$distance
      if (!is.null(method_params$replace)) args$replace <- method_params$replace
      if (!is.null(method_params$subclass)) args$subclass <- method_params$subclass
      if (!is.null(method_params$distance.options)) args$distance.options <- method_params$distance.options
      
      do.call(matchit, args)
      
    } else if (method_params$package == "weightit") {
      # WeightIt methods
      args <- list(
        formula = ps_formula,
        data = data,
        method = method_params$method,
        estimand = method_params$estimand
      )
      
      # Add method-specific parameters
      if (!is.null(method_params$over)) args$over <- method_params$over
      
      do.call(weightit, args)
      
    } else {
      stop(paste("Unknown package:", method_params$package))
    }
  }, error = function(e) {
    cat(paste0("  ERROR in ", method_name, " (imp ", imputation_id, "): ", e$message, "\n"))
    return(NULL)
  })
  
  # Save to cache
  if (!is.null(result) && use_cache) {
    saveRDS(result, cache_file)
  }
  
  return(result)
}

# Function to process all imputations for a single method
process_method_all_imputations <- function(method_params, imputed_datasets, ps_formula, 
                                          cache_dir, use_cache = TRUE, n_cores = 1) {
  n_imp <- length(imputed_datasets)
  method_name <- method_params$name
  
  log_message(paste0("Processing method: ", method_name, " across ", n_imp, " imputations"))
  
  if (n_cores > 1 && .Platform$OS.type == "unix") {
    # Parallel processing for Unix/Mac
    results <- mclapply(
      1:n_imp,
      function(i) process_ps_method(i, imputed_datasets[[i]], method_params, ps_formula, cache_dir, use_cache),
      mc.cores = n_cores
    )
  } else if (n_cores > 1 && .Platform$OS.type != "unix") {
    # Parallel processing for Windows
    cl <- makeCluster(n_cores)
    clusterExport(cl, c("process_ps_method", "ps_formula", "cache_dir", "use_cache", "method_params"),
                  envir = environment())
    clusterEvalQ(cl, {
      library(MatchIt)
      library(WeightIt)
    })
    
    results <- parLapply(cl, 1:n_imp, function(i) {
      process_ps_method(i, imputed_datasets[[i]], method_params, ps_formula, cache_dir, use_cache)
    })
    
    stopCluster(cl)
  } else {
    # Sequential processing
    results <- lapply(
      1:n_imp,
      function(i) process_ps_method(i, imputed_datasets[[i]], method_params, ps_formula, cache_dir, use_cache)
    )
  }
  
  return(results)
}
```

## Run Propensity Score Methods

```{r run-ps-methods}
# Process only the specified number of imputations
imputed_datasets_subset <- imputed_datasets[1:min(max_imputations_to_process, length(imputed_datasets))]

# Initialize results storage
all_ps_results <- list()

# Process each method sequentially (to manage memory)
for (method_params in ps_methods) {
  method_name <- method_params$name
  log_message(paste0("\n", paste(rep("=", 50), collapse = "")))
  log_message(paste0("Starting method: ", method_name))
  
  # Process all imputations for this method
  method_results <- process_method_all_imputations(
    method_params = method_params,
    imputed_datasets = imputed_datasets_subset,
    ps_formula = ps_formula,
    cache_dir = cache_dir,
    use_cache = use_cache,
    n_cores = max_cores_for_ps
  )
  
  all_ps_results[[method_name]] <- method_results
  
  # Clean up memory after each method
  gc()
  
  log_message(paste0("Completed method: ", method_name))
}

log_message("All PS methods completed")
```

## Load twang_gbm Results from Reanalysis-4

```{r load-twang}
# Load pre-computed twang_gbm results
twang_cache_dir <- file.path(reanalysis_data_dir, "ps_cache")
twang_results <- list()

for (i in 1:max_imputations_to_process) {
  twang_file <- file.path(twang_cache_dir, paste0("ps_imp_", i, ".rds"))
  if (file.exists(twang_file)) {
    twang_results[[i]] <- readRDS(twang_file)
    log_message(paste0("Loaded twang_gbm results for imputation ", i))
  } else {
    log_message(paste0("WARNING: twang_gbm results not found for imputation ", i))
    twang_results[[i]] <- NULL
  }
}

# Add to all results
all_ps_results[["twang_gbm"]] <- twang_results

log_message(paste0("Loaded twang_gbm results for ", sum(!sapply(twang_results, is.null)), " imputations"))
```

## Outcome Analysis

```{r outcome-analysis}
# Get outcome formula from functions.R
outcome_formulas <- get_analysis_formulas_reanalysis()
outcome_formula <- outcome_formulas$full

log_message("Starting outcome analysis for all methods")

# Function to fit outcome model for a single method and imputation
fit_outcome_model <- function(ps_result, data, formula, method_name) {
  if (is.null(ps_result)) {
    return(NULL)
  }
  
  tryCatch({
    if (method_name == "twang_gbm") {
      # twang weights
      weights <- ps_result$w[, "es.mean.ATT"]
      data$weights <- weights
      design <- svydesign(ids = ~e_mrn_deidentified, weights = ~weights, data = data)
      
    } else if (inherits(ps_result, "matchit")) {
      # MatchIt results
      if (grepl("subclass", method_name)) {
        # Subclassification
        matched_data <- match.data(ps_result, subclass = "subclass")
        design <- svydesign(ids = ~e_mrn_deidentified, data = matched_data, weights = ~weights)
        design <- postStratify(design, ~subclass, xtabs(~subclass, data = matched_data))
      } else {
        # Matching
        matched_data <- get_matches(ps_result)
        design <- svydesign(ids = ~e_mrn_deidentified, data = matched_data, weights = ~1)
      }
      
    } else if (inherits(ps_result, "weightit")) {
      # WeightIt results
      weights <- ps_result$weights
      data$weights <- weights
      design <- svydesign(ids = ~e_mrn_deidentified, weights = ~weights, data = data)
      
    } else {
      stop("Unknown result type")
    }
    
    # Fit the model
    model <- svyglm(formula, design = design, family = quasibinomial())
    
    # Extract coefficient for lapse
    coef_lapse <- coef(summary(model))["ever_lapse_binary", ]
    
    return(list(
      estimate = coef_lapse["Estimate"],
      se = coef_lapse["Std. Error"],
      model = model
    ))
    
  }, error = function(e) {
    cat(paste0("  Error fitting model for ", method_name, ": ", e$message, "\n"))
    return(NULL)
  })
}

# Process outcome models for all methods and imputations
outcome_results <- list()

for (method_name in names(all_ps_results)) {
  log_message(paste0("Fitting outcome models for ", method_name))
  
  method_outcomes <- list()
  ps_results <- all_ps_results[[method_name]]
  
  for (i in 1:length(ps_results)) {
    if (!is.null(ps_results[[i]])) {
      outcome <- fit_outcome_model(
        ps_result = ps_results[[i]],
        data = imputed_datasets_subset[[i]],
        formula = outcome_formula,
        method_name = method_name
      )
      method_outcomes[[i]] <- outcome
    }
  }
  
  outcome_results[[method_name]] <- method_outcomes
}

log_message("Outcome analysis completed")
```

## Pool Results Using Rubin's Rules

```{r pool-results}
# Function to pool results across imputations using Rubin's rules
pool_results <- function(method_outcomes) {
  # Filter out NULL results
  valid_outcomes <- method_outcomes[!sapply(method_outcomes, is.null)]
  
  if (length(valid_outcomes) == 0) {
    return(NULL)
  }
  
  # Extract estimates and SEs
  estimates <- sapply(valid_outcomes, function(x) x$estimate)
  ses <- sapply(valid_outcomes, function(x) x$se)
  
  # Remove any NA values
  valid_idx <- !is.na(estimates) & !is.na(ses)
  estimates <- estimates[valid_idx]
  ses <- ses[valid_idx]
  
  if (length(estimates) == 0) {
    return(NULL)
  }
  
  m <- length(estimates)  # Number of imputations
  
  # Pooled estimate (average of estimates)
  pooled_estimate <- mean(estimates)
  
  # Within-imputation variance (average of squared SEs)
  within_var <- mean(ses^2)
  
  # Between-imputation variance
  between_var <- var(estimates)
  
  # Total variance using Rubin's rules
  total_var <- within_var + between_var + (between_var / m)
  pooled_se <- sqrt(total_var)
  
  # Degrees of freedom (Barnard-Rubin adjustment)
  lambda <- (between_var + between_var/m) / total_var
  df_old <- (m - 1) / lambda^2
  
  # For large samples, use df_old
  df <- df_old
  
  # Calculate confidence intervals and p-value
  t_stat <- pooled_estimate / pooled_se
  p_value <- 2 * pt(abs(t_stat), df = df, lower.tail = FALSE)
  ci_lower <- pooled_estimate - qt(0.975, df) * pooled_se
  ci_upper <- pooled_estimate + qt(0.975, df) * pooled_se
  
  # Convert to odds ratio scale
  or <- exp(pooled_estimate)
  or_ci_lower <- exp(ci_lower)
  or_ci_upper <- exp(ci_upper)
  
  return(list(
    estimate = pooled_estimate,
    se = pooled_se,
    or = or,
    or_ci_lower = or_ci_lower,
    or_ci_upper = or_ci_upper,
    p_value = p_value,
    n_imputations = m,
    df = df
  ))
}

# Pool results for each method
pooled_results <- list()

for (method_name in names(outcome_results)) {
  pooled <- pool_results(outcome_results[[method_name]])
  if (!is.null(pooled)) {
    pooled$method <- method_name
    pooled_results[[method_name]] <- pooled
    log_message(paste0("Pooled results for ", method_name, ": OR = ", 
                      round(pooled$or, 3), " (", round(pooled$or_ci_lower, 3), 
                      ", ", round(pooled$or_ci_upper, 3), ")"))
  } else {
    log_message(paste0("WARNING: Could not pool results for ", method_name))
  }
}

# Save pooled results
saveRDS(pooled_results, file.path(reanalysis_data_dir, "alternative_ps_results_pooled.rds"))
log_message("Saved pooled results")
```

## Create Forest Plot

```{r forest-plot}
# Prepare data for forest plot
forest_data <- do.call(rbind, lapply(names(pooled_results), function(method) {
  res <- pooled_results[[method]]
  data.frame(
    method = method,
    or = res$or,
    lower = res$or_ci_lower,
    upper = res$or_ci_upper,
    p_value = res$p_value,
    stringsAsFactors = FALSE
  )
}))

# Order by odds ratio
forest_data <- forest_data[order(forest_data$or), ]

# Create more readable method names
forest_data$method_label <- gsub("_", " ", forest_data$method)
forest_data$method_label <- gsub("nearest ", "NN-", forest_data$method_label)
forest_data$method_label <- gsub("subclass ", "Subclass-", forest_data$method_label)
forest_data$method_label <- tools::toTitleCase(forest_data$method_label)

# Create forest plot using ggplot2
library(ggplot2)

forest_plot <- ggplot(forest_data, aes(x = or, y = reorder(method_label, or))) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "gray50") +
  geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2) +
  geom_point(size = 3, shape = 18) +
  scale_x_continuous(
    trans = "log",
    breaks = c(0.5, 0.75, 1, 1.5, 2, 3),
    labels = c("0.5", "0.75", "1.0", "1.5", "2.0", "3.0")
  ) +
  labs(
    x = "Odds Ratio (95% CI)",
    y = "Propensity Score Method",
    title = "Effect of Lapse on Vision Loss Across PS Methods",
    subtitle = paste0("Based on ", max_imputations_to_process, " imputations with Rubin's pooling")
  ) +
  theme_minimal() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.y = element_text(hjust = 0)
  )

print(forest_plot)

# Save plot
ggsave(
  filename = file.path(reanalysis_data_dir, "forest_plot_ps_methods.png"),
  plot = forest_plot,
  width = 10,
  height = 8,
  dpi = 300
)

log_message("Forest plot saved")
```

## Summary Tables

```{r summary-tables}
# Create detailed summary table
summary_table <- do.call(rbind, lapply(names(pooled_results), function(method) {
  res <- pooled_results[[method]]
  data.frame(
    Method = method,
    `OR` = sprintf("%.3f", res$or),
    `95% CI` = sprintf("(%.3f, %.3f)", res$or_ci_lower, res$or_ci_upper),
    `P-value` = sprintf("%.4f", res$p_value),
    `N Imputations` = res$n_imputations,
    stringsAsFactors = FALSE
  )
}))

# Sort by OR
summary_table <- summary_table[order(as.numeric(gsub("([0-9.]+)", "\\1", summary_table$OR))), ]

kable(summary_table, 
      caption = "Pooled Effect Estimates Across Propensity Score Methods",
      col.names = c("Method", "Odds Ratio", "95% CI", "P-value", "N Imputations"),
      align = c("l", "r", "r", "r", "r"))

# Save to CSV
write.csv(summary_table, 
          file.path(reanalysis_data_dir, "ps_methods_comparison_table.csv"),
          row.names = FALSE)

log_message("Summary table saved")

# Create comparison with twang_gbm as reference
if ("twang_gbm" %in% names(pooled_results)) {
  ref_or <- pooled_results[["twang_gbm"]]$or
  
  comparison_table <- do.call(rbind, lapply(names(pooled_results), function(method) {
    res <- pooled_results[[method]]
    data.frame(
      Method = method,
      OR = res$or,
      `Relative to twang` = sprintf("%.1f%%", 100 * (res$or / ref_or - 1)),
      stringsAsFactors = FALSE
    )
  }))
  
  kable(comparison_table,
        caption = "Comparison of Methods Relative to twang_gbm",
        col.names = c("Method", "Odds Ratio", "% Difference from twang_gbm"),
        align = c("l", "r", "r"))
}
```

## Summary

This analysis compared `r length(pooled_results)` different propensity score methods across `r max_imputations_to_process` multiply imputed datasets. Results were pooled using Rubin's rules to account for between-imputation variability.

Key findings:
- Method with smallest effect: `r names(pooled_results)[which.min(sapply(pooled_results, function(x) x$or))]`
- Method with largest effect: `r names(pooled_results)[which.max(sapply(pooled_results, function(x) x$or))]`
- Range of odds ratios: `r sprintf("%.3f to %.3f", min(sapply(pooled_results, function(x) x$or)), max(sapply(pooled_results, function(x) x$or)))`

