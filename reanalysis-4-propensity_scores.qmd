---
title: "Reanalysis Step 4: Propensity Score Analysis with Twang"
subtitle: "Calculating IPTW weights using GBM across imputed datasets"
author: "Erik Westlund"
date: "`r Sys.Date()`"
format: html
---

## Setup

```{r setup}
#| include: false
source("dependencies.R")
source("functions.R")  # Load logging functions and other utilities
setup_analysis(seed = 2025)

library(twang)
library(ggplot2)
library(gridExtra)
library(knitr)
library(parallel)
library(doParallel)

# ANALYSIS MODE: Use same as imputation
analysis_mode <- "final"  # Should match imputation mode

# PARALLEL PROCESSING CONFIGURATION
max_cores_for_ps <- 3  # Maximum cores to use for propensity score estimation (memory-intensive)

# CACHING CONFIGURATION
use_cache <- FALSE  # Set to FALSE to force re-computation
cache_dir <- file.path(reanalysis_data_dir, "ps_cache")
if (!dir.exists(cache_dir)) {
  dir.create(cache_dir, recursive = TRUE)
}

# Set parameters based on mode
if (analysis_mode == "test") {
  max_imputations_to_process <- 2  # Process only 2 imputations in test mode
  max_imputations_for_diagnostics <- 2  # Show diagnostics for 2 imputations in test
  n_trees <- 1000  # Fewer trees for testing
} else {
  max_imputations_to_process <- NULL  # Process all available imputations
  max_imputations_for_diagnostics <- 5  # Show diagnostics for up to 5 imputations in final
  n_trees <- 3000  # More trees for final analysis
}

# Set up parallel processing
# Reduce cores for memory-intensive GBM models
available_cores <- detectCores()
# Use fewer cores to avoid memory issues - each GBM model needs substantial memory
# Don't subtract 1 from max_cores_for_ps since it's already the desired limit
n_cores <- if (available_cores > max_cores_for_ps) max_cores_for_ps else max(1, available_cores - 1)
log_message(paste0("Parallel processing: Using ", n_cores, " cores (of ", available_cores, " available) to manage memory usage"))

```

**Analysis mode:** `r analysis_mode`  
**Imputations to process:** `r if(!is.null(max_imputations_to_process)) max_imputations_to_process else "all available"`  
**Diagnostics shown for:** `r max_imputations_for_diagnostics` **imputations**  
**GBM trees:** `r n_trees`

## Load Imputed Datasets

```{r load-data}
imputed_datasets <- readRDS(file.path(reanalysis_data_dir, "imputed_datasets.rds"))
n_available <- length(imputed_datasets)

# Determine how many imputations to actually process
if (!is.null(max_imputations_to_process)) {
  n_imputations <- min(max_imputations_to_process, n_available)
  
  if (n_imputations < n_available) {
    imputed_datasets <- imputed_datasets[1:n_imputations]
  }
} else {
  n_imputations <- n_available
}

```

**Available imputed datasets:** `r n_available`  
**Processing:** `r n_imputations` **datasets** `r if(analysis_mode == "test") "(test mode limit)" else "(all available)"`

```{r continue-load}

variable_lists <- readRDS(file.path(reanalysis_data_dir, "variable_lists.rds"))
```

## Define Propensity Score Formula

```{r ps-formula}
# Use reanalysis formula that includes DR severity
# This is more comprehensive than the original formula

ps_formula_original <- get_matching_formula()
ps_formula_reanalysis <- get_matching_formula_reanalysis()

```

**Original propensity score formula (without DR severity):**
```{r print-formula-original}
print(ps_formula_original)
```

**Reanalysis propensity score formula (with DR severity):**
```{r print-formula-reanalysis}
print(ps_formula_reanalysis)

# Use the reanalysis formula that includes dr_severity
ps_formula <- ps_formula_reanalysis
log_message("Using reanalysis formula with dr_severity for propensity score estimation.")
```

## Run Twang Propensity Score Models

Following the approach in twang-analysis.Rmd, we'll use GBM via twang package to estimate propensity scores.

### Model Parameters
- **Method:** Twang GBM
- **n.trees:** Set based on analysis mode (1000 for test, 3000 for final)
- **interaction.depth:** 3
- **shrinkage:** 0.01
- **estimand:** ATT
- **stop.method:** es.mean (effect size only)

```{r ps-models}
# Initialize logging
log_file <- init_log("propensity_scores")
log_message(paste0("Analysis mode: ", analysis_mode))
log_message(paste0("Number of imputations to process: ", n_imputations))
log_message(paste0("Cache directory: ", cache_dir))
log_message(paste0("Use cache: ", use_cache))

# Function to process a single imputation
process_imputation <- function(i, imp_datasets, formula, trees, cache_path, use_caching, node_id, log_file_path) {
  # Recreate variables inside function to avoid scoping issues
  library(twang)
  
  # Log node start
  start_msg <- paste0("Node ", node_id, " started: Processing imputation ", i)
  cat(paste0(Sys.time(), " - ", start_msg, "\n"), file = log_file_path, append = TRUE)
  
  # Check for cached result
  cache_file <- file.path(cache_path, paste0("twang_imp_", i, ".rds"))
  
  if (use_caching && file.exists(cache_file)) {
    cached_result <- readRDS(cache_file)
    weights <- cached_result$weights
    
    # Log cache hit
    cache_msg <- paste0("Node ", node_id, " completed: Imputation ", i, " loaded from cache")
    cat(paste0(Sys.time(), " - ", cache_msg, "\n"), file = log_file_path, append = TRUE)
    
    return(list(
      imputation = i,
      node_id = node_id,
      result = cached_result,
      trees_used = cached_result$n_trees_used,
      weight_min = round(min(weights), 3),
      weight_max = round(max(weights), 3),
      cached = TRUE,
      time_mins = 0
    ))
  } else {
    imp_start <- Sys.time()
    
    # Get the data for this imputation
    imp_data <- as.data.frame(imp_datasets[[i]])
    
    # Ensure ever_lapse_binary is properly formatted as numeric 0/1
    if ("ever_lapse_binary" %in% names(imp_data)) {
      if (is.factor(imp_data$ever_lapse_binary)) {
        # Convert factor to numeric via character to avoid level issues
        imp_data$ever_lapse_binary <- as.numeric(as.character(imp_data$ever_lapse_binary))
      } else if (is.character(imp_data$ever_lapse_binary)) {
        imp_data$ever_lapse_binary <- as.numeric(imp_data$ever_lapse_binary)
      } else if (is.logical(imp_data$ever_lapse_binary)) {
        imp_data$ever_lapse_binary <- as.numeric(imp_data$ever_lapse_binary)
      }
      
      # Ensure values are 0 or 1
      if (any(!imp_data$ever_lapse_binary %in% c(0, 1, NA))) {
        stop(paste("ever_lapse_binary variable contains values other than 0, 1, or NA in imputation", i))
      }
    } else {
      stop(paste("ever_lapse_binary variable not found in imputation", i))
    }
    
    # Run twang ps model
    twang_model <- ps(
      formula = formula,
      data = imp_data,
      n.trees = trees,
      interaction.depth = 3,
      shrinkage = 0.01,
      estimand = "ATT",
      stop.method = c("es.mean"),
      verbose = FALSE
    )
    
    # Extract weights for ATT using es.mean
    weights <- get.weights(twang_model, stop.method = "es.mean")
    
    # Store results
    result <- list(
      model = twang_model,
      weights = weights,
      n_trees_used = twang_model$desc$es.mean$n.trees
    )
    
    # Save to cache
    saveRDS(result, cache_file)
    
    imp_time <- round(as.numeric(difftime(Sys.time(), imp_start, units = "mins")), 2)
    
    # Log completion
    complete_msg <- paste0("Node ", node_id, " completed: Imputation ", i, " fitted in ", imp_time, " minutes")
    cat(paste0(Sys.time(), " - ", complete_msg, "\n"), file = log_file_path, append = TRUE)
    
    return(list(
      imputation = i,
      node_id = node_id,
      result = result,
      trees_used = result$n_trees_used,
      weight_min = round(min(weights), 3),
      weight_max = round(max(weights), 3),
      cached = FALSE,
      time_mins = imp_time
    ))
  }
}

# Create assignments of imputations to node IDs
# Each imputation gets a unique sequential ID for tracking
imputation_node_assignments <- data.frame(
  imputation = 1:n_imputations,
  node_id = 1:n_imputations  # Simple sequential numbering: 1, 2, 3, 4, etc.
)

# Run parallel processing
log_message(paste0("Starting parallel processing of ", n_imputations, " imputations using ", n_cores, " cores"))
log_message(paste0("Node assignments: ", paste(imputation_node_assignments$node_id, collapse = ", ")))
parallel_start <- Sys.time()

# Use mclapply on Unix/Mac, parLapply on Windows
if (.Platform$OS.type == "unix") {
  # Unix/Mac - use mclapply
  parallel_results <- mclapply(
    1:n_imputations,
    function(i) {
      node_id <- imputation_node_assignments$node_id[i]
      process_imputation(i, imputed_datasets, ps_formula, n_trees, cache_dir, use_cache, node_id, log_file)
    },
    mc.cores = n_cores
  )
} else {
  # Windows - use parLapply
  cl <- makeCluster(n_cores)
  
  # Export necessary objects to cluster
  clusterExport(cl, c(
    "process_imputation", "imputed_datasets", "ps_formula", 
    "n_trees", "cache_dir", "use_cache", "imputation_node_assignments", "log_file"
  ), envir = environment())
  
  # Load required packages on each worker
  clusterEvalQ(cl, {
    library(twang)
  })
  
  # Create wrapper function for parLapply
  parallel_results <- parLapply(cl, 1:n_imputations, function(i) {
    node_id <- imputation_node_assignments$node_id[i]
    process_imputation(i, imputed_datasets, ps_formula, n_trees, cache_dir, use_cache, node_id, log_file)
  })
  
  stopCluster(cl)
}

parallel_time <- round(as.numeric(difftime(Sys.time(), parallel_start, units = "mins")), 2)
log_message(paste0("Parallel processing completed in ", parallel_time, " minutes"))

# Extract results and build progress dataframe
twang_results <- list()
progress_df <- data.frame(
  Imputation = integer(),
  Node_ID = integer(),
  Trees_Used = integer(),
  Weight_Min = numeric(),
  Weight_Max = numeric(),
  Cached = logical(),
  Time_Minutes = numeric()
)

for (res in parallel_results) {
  twang_results[[res$imputation]] <- res$result
  
  progress_df <- rbind(progress_df, data.frame(
    Imputation = res$imputation,
    Node_ID = res$node_id,
    Trees_Used = res$trees_used,
    Weight_Min = res$weight_min,
    Weight_Max = res$weight_max,
    Cached = res$cached,
    Time_Minutes = res$time_mins
  ))
  
  # Log individual result summary
  if (res$cached) {
    log_message(paste0("Imputation ", res$imputation, " (Node ", res$node_id, "): Loaded from cache"))
  } else {
    log_message(paste0("Imputation ", res$imputation, " (Node ", res$node_id, "): Fitted in ", res$time_mins, " minutes"))
  }
}

# Sort progress_df by imputation number
progress_df <- progress_df[order(progress_df$Imputation), ]

# Display progress summary with caching and timing information
kable(progress_df, 
      caption = paste0("Twang Model Fitting Progress (Total time: ", parallel_time, " minutes using ", n_cores, " cores)"),
      col.names = c("Imputation", "Node ID", "Trees Used", "Min Weight", "Max Weight", "From Cache", "Time (min)"))
```

## Balance Assessment

```{r balance-assessment}
# Extract balance summaries from twang models
balance_summaries <- list()

for (i in 1:n_imputations) {
  # Get the twang model
  twang_model <- twang_results[[i]]$model
  
  weights <- twang_results[[i]]$weights
  treated <- imputed_datasets[[i]]$ever_lapse_binary == 1
  
  balance_summaries[[i]] <- data.frame(
    Imputation = i,
    N_Treated = sum(treated),
    N_Control = sum(!treated),
    Mean_Weight_Treated = mean(weights[treated]),
    Mean_Weight_Control = mean(weights[!treated]),
    Max_Weight = max(weights)
  )
  
  if (i <= max_imputations_for_diagnostics) {
    # Balance summary will be shown in diagnostics section
  }
}

balance_summary_all <- do.call(rbind, balance_summaries)

```

### Weight Summary Across All Imputations

```{r weight-summary-table}
kable(balance_summary_all, 
      caption = "Weight distribution by imputation",
      digits = 3)

# Calculate average across imputations
avg_weights <- data.frame(
  Avg_N_Treated = mean(balance_summary_all$N_Treated),
  Avg_N_Control = mean(balance_summary_all$N_Control),
  Avg_Weight_Treated = mean(balance_summary_all$Mean_Weight_Treated),
  Avg_Weight_Control = mean(balance_summary_all$Mean_Weight_Control),
  Max_Weight_Overall = max(balance_summary_all$Max_Weight)
)

```

### Average Weights Across Imputations

```{r avg-weights-table}
kable(avg_weights,
      caption = "Average weight statistics across all imputations",
      digits = 3)

```

### Weight Stability Check

```{r weight-stability-check}
if (avg_weights$Max_Weight_Overall > 10) {
  print("Warning: Maximum weight > 10, indicating potential instability")
} else if (avg_weights$Max_Weight_Overall > 5) {
  print("Note: Maximum weight > 5, monitor for stability")
} else {
  print("Weights appear stable (max < 5)")
}
```

## Diagnostics for Sample of Imputations

Showing diagnostic plots for the first **`r min(max_imputations_for_diagnostics, n_imputations)`** imputations.

```{r diagnostics-plots}
#| fig-height: 8
#| fig-width: 10

n_to_show <- min(max_imputations_for_diagnostics, n_imputations)
imps_to_show <- 1:n_to_show
```

```{r diagnostics-loop}
#| results: "asis"

# Generate diagnostic plots for selected imputations
for (i in imps_to_show) {
  log_message(paste0("Processing diagnostics for Imputation ", i))
  cat("\n### Imputation", i, "\n\n")
  
  twang_model <- twang_results[[i]]$model
  weights <- twang_results[[i]]$weights
  
  # Create dataset with weights for cobalt
  data_with_weights <- imputed_datasets[[i]]
  data_with_weights$weights <- weights
  
  cat("#### Convergence (Iteration Plot)\n\n")
  # Plot 1: Shows how balance metrics change across iterations
  print(plot(twang_model, plots = 1))
  cat("\n\n")
  
  cat("#### Weight Distribution (Boxplot)\n\n")
  # Plot 2: Shows distribution of weights
  print(plot(twang_model, plots = 2))
  cat("\n\n")
  
  cat("#### Covariate Balance (Standardized Mean Differences)\n\n")
  # Plot 3: Shows balance for each covariate
  print(plot(twang_model, plots = 3))
  cat("\n\n")
  
  cat("#### Love Plot\n\n")
  # Use cobalt for better visualization
  love_plot <- love.plot(
    bal.tab(ps_formula, 
            data = data_with_weights,
            weights = "weights",
            estimand = "ATT",
            s.d.denom = "treated"),
    thresholds = c(m = 0.1),
    title = paste("Covariate Balance - Imputation", i)
  )
  print(love_plot)
  cat("\n\n")
  
  cat("#### Weight Histogram\n\n")
  # Plot 6: Histogram of weight proportions
  print(plot(twang_model, plots = 6))
  cat("\n\n")
  
  cat("#### Balance Summary Table\n\n")
  
  # Get balance statistics from the twang model
  # The bal.table function gives us the detailed balance statistics
  tryCatch({
    bal_table <- bal.table(twang_model, digits = 4)
    
    # Extract the summary statistics for the optimal stopping point
    # twang stores these in the desc slot
    es_mean_stats <- twang_model$desc$es.mean
    
    # Safely extract values with error checking
    safe_round <- function(x, digits = 4) {
      if (is.null(x) || !is.numeric(x)) return(NA)
      return(round(x, digits))
    }
    
    # Create a clean summary table
    summary_df <- data.frame(
      Metric = c("N Treated", "N Control", 
                 "ESS Treated", "ESS Control",
                 "Max ES", "Mean ES", "Max KS",
                 "Iterations"),
      Unweighted = c(
        es_mean_stats$n.treat,
        es_mean_stats$n.ctrl,
        es_mean_stats$n.treat,  # ESS = n for unweighted
        es_mean_stats$n.ctrl,    # ESS = n for unweighted
        safe_round(bal_table$unw$max.es, 4),
        safe_round(bal_table$unw$mean.es, 4),
        safe_round(bal_table$unw$max.ks, 4),
        NA
      ),
      Weighted = c(
        es_mean_stats$n.treat,
        es_mean_stats$n.ctrl,
        safe_round(es_mean_stats$ess.treat, 2),
        safe_round(es_mean_stats$ess.ctrl, 2),
        safe_round(bal_table$es.mean$max.es, 4),
        safe_round(bal_table$es.mean$mean.es, 4),
        safe_round(bal_table$es.mean$max.ks, 4),
        es_mean_stats$n.trees
      )
    )
    
    kable(summary_df, 
          caption = paste("Balance Summary for Imputation", i),
          align = c('l', 'r', 'r'))
    
  }, error = function(e) {
    # Fallback to simpler summary if bal.table fails
    log_message("Note: Detailed balance table unavailable. Showing basic summary.")
    
    # Use the summary method which is more reliable
    sum_output <- summary(twang_model)
    
    # Try to extract key metrics from the summary
    if (!is.null(sum_output)) {
      print(sum_output)
    } else {
      log_message("Balance statistics not available for this imputation.")
    }
  })
  
  # Spacing
}
```

## Propensity Score Distribution

```{r ps-distribution}
#| fig-height: 6
#| fig-width: 10

# Create weight distribution summary
weight_dist_summary <- data.frame(
  Imputation = integer(),
  Control_Mean = numeric(),
  Control_Max = numeric(),
  Treated_Mean = numeric()
)

# Plot PS distribution for first few imputations
for (i in imps_to_show) {
  
  twang_model <- twang_results[[i]]$model
  weights <- twang_results[[i]]$weights
  treated <- imputed_datasets[[i]]$ever_lapse_binary == 1
  
  par(mfrow = c(1, 2))
  
  # Control group weights
  hist(weights[!treated], 
       main = paste("Weights: Control - Imputation", i),
       xlab = "Weight",
       col = "lightblue",
       breaks = 20)
  
  # Treated group weights (should all be 1 for ATT)
  hist(weights[treated],
       main = paste("Weights: Treated - Imputation", i),
       xlab = "Weight", 
       col = "lightcoral",
       breaks = 20)
  
  par(mfrow = c(1, 1))
  
  # Store weight summary
  weight_dist_summary <- rbind(weight_dist_summary, data.frame(
    Imputation = i,
    Control_Mean = round(mean(weights[!treated]), 3),
    Control_Max = round(max(weights[!treated]), 3),
    Treated_Mean = round(mean(weights[treated]), 3)
  ))
}

# Display weight distribution summary
kable(weight_dist_summary, 
      caption = "Weight Distribution by Imputation",
      col.names = c("Imputation", "Control Mean", "Control Max", "Treated Mean (should be 1.0 for ATT)"))
```


## Save Propensity Score Results

```{r save-results}
# Extract weights from all twang models for easy access in later steps
twang_weights <- list()
for (i in 1:n_imputations) {
  twang_weights[[i]] <- twang_results[[i]]$weights
}

ps_output <- list(
  twang_results = twang_results,
  weight_summary = balance_summary_all,
  avg_weights = avg_weights,
  formula_used = ps_formula,
  n_imputations = n_imputations,
  method = "twang_gbm",
  n_trees = n_trees,
  analysis_mode = analysis_mode
)

saveRDS(ps_output, file.path(reanalysis_data_dir, "ps_results_twang.rds"))
saveRDS(twang_weights, file.path(reanalysis_data_dir, "twang_weights.rds"))

```

**Propensity score results saved to:** `r file.path(reanalysis_data_dir, "ps_results_twang.rds")`
**Weights saved to:** `r file.path(reanalysis_data_dir, "twang_weights.rds")`
**Cache directory:** `r cache_dir`

```{r cache-summary}
# Check cache status
cache_files <- list.files(cache_dir, pattern = "twang_imp_.*\\.rds", full.names = FALSE)
n_cached <- length(cache_files)

cache_status_msg <- paste0("Cache status: ", n_cached, " of ", n_imputations, " imputations cached")
log_message(cache_status_msg)

if (n_cached > 0) {
  log_message("To force re-computation, set use_cache <- FALSE in the setup chunk")
}

# Finalize logging
final_log <- finalize_log(success = TRUE)
```

**Log file location:** `r final_log`

To monitor the log during execution:
```bash
tail -f `r final_log`
```

## Summary

### Analysis Configuration

**Method:** Generalized Boosted Models (GBM) via twang  
**Estimand:** ATT (Average Treatment Effect on the Treated)  
**Stop Method:** es.mean (effect size minimization)  

```{r summary-config}
config_summary <- data.frame(
  Parameter = c("Analysis Mode", "Imputations Processed", "Trees per Model"),
  Value = c(analysis_mode, n_imputations, n_trees)
)

kable(config_summary, caption = "Run Configuration")
```

### Formula Components

The enhanced propensity score formula includes:
- DR severity (person_dr)
- Treatment type variables
- All standard covariates from original analysis

```{r summary-formula}
print(ps_formula)
```

### Weight Statistics Summary

```{r summary-weights}
weight_stats <- data.frame(
  Statistic = c("Mean Weight (Treated)", "Mean Weight (Control)", "Maximum Weight"),
  Value = c(round(avg_weights$Avg_Weight_Treated, 3),
           round(avg_weights$Avg_Weight_Control, 3),
           round(avg_weights$Max_Weight_Overall, 2))
)

kable(weight_stats, caption = "Weight Statistics (Averaged Across Imputations)")
```
