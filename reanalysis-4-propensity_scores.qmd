---
title: "Reanalysis Step 4: Propensity Score Analysis with Twang"
subtitle: "Calculating IPTW weights using GBM across imputed datasets"
author: "Erik Westlund"
date: "`r Sys.Date()`"
format: html
---

## Setup

```{r setup}
#| include: false
source("dependencies.R")
source("functions.R")  # Load logging functions and other utilities
setup_analysis(seed = 2025)

library(twang)
library(ggplot2)
library(gridExtra)
library(knitr)

# ANALYSIS MODE: Use same as imputation
analysis_mode <- "final"  # Should match imputation mode

# SEQUENTIAL PROCESSING (parallel clustering removed for stability)

# CACHING CONFIGURATION
use_cache <- TRUE  # Set to TRUE to use cache, FALSE to force re-computation
cache_max_age_days <- 2  # Cache files older than this many days will be recomputed
cache_dir <- file.path(reanalysis_data_dir, "ps_cache")
if (!dir.exists(cache_dir)) {
  dir.create(cache_dir, recursive = TRUE)
}

# Get list of existing cache files with their modification times
if (use_cache) {
  cache_files <- list.files(cache_dir, pattern = "^twang_gbm_imp.*\\.rds$", full.names = TRUE)
  if (length(cache_files) > 0) {
    cache_info <- file.info(cache_files)
    cache_info$file <- basename(rownames(cache_info))
    cache_info$age_days <- as.numeric(difftime(Sys.time(), cache_info$mtime, units = "days"))
    log_message(paste0("Found ", nrow(cache_info), " cached files in ", cache_dir))
    log_message(paste0("Cache staleness threshold: ", cache_max_age_days, " days"))
  } else {
    cache_info <- NULL
    log_message("No cached files found")
  }
} else {
  log_message("Cache disabled - will recompute all imputations")
}

# Set parameters based on mode
if (analysis_mode == "test") {
  max_imputations_to_process <- 2  # Process only 2 imputations in test mode
  max_imputations_for_diagnostics <- 2  # Show diagnostics for 2 imputations in test
  n_trees <- 1000  # Fewer trees for testing
} else {
  max_imputations_to_process <- NULL  # Process all available imputations
  max_imputations_for_diagnostics <- 5  # Show diagnostics for up to 5 imputations in final
  n_trees <- 3000  # More trees for final analysis
}

# Sequential processing - no parallel cores used
log_message("Using sequential processing (parallel clustering removed for stability)")

```

**Analysis mode:** `r analysis_mode`  
**Imputations to process:** `r if(!is.null(max_imputations_to_process)) max_imputations_to_process else "all available"`  
**Diagnostics shown for:** `r max_imputations_for_diagnostics` **imputations**  
**GBM trees:** `r n_trees`

## Load Imputed Datasets

```{r load-data}
imputed_datasets <- readRDS(file.path(reanalysis_data_dir, "imputed_datasets.rds"))
n_available <- length(imputed_datasets)

# Determine how many imputations to actually process
if (!is.null(max_imputations_to_process)) {
  n_imputations <- min(max_imputations_to_process, n_available)
  
  if (n_imputations < n_available) {
    imputed_datasets <- imputed_datasets[1:n_imputations]
  }
} else {
  n_imputations <- n_available
}

```

**Available imputed datasets:** `r n_available`  
**Processing:** `r n_imputations` **datasets** `r if(analysis_mode == "test") "(test mode limit)" else "(all available)"`

```{r continue-load}

variable_lists <- readRDS(file.path(reanalysis_data_dir, "variable_lists.rds"))
```

## Define Propensity Score Formula

```{r ps-formula}
# Use reanalysis formula that includes DR severity
# This is more comprehensive than the original formula

ps_formula_original <- get_matching_formula()
ps_formula_reanalysis <- get_matching_formula_reanalysis()

```

**Original propensity score formula (without DR severity):**
```{r print-formula-original}
print(ps_formula_original)
```

**Reanalysis propensity score formula (with DR severity):**
```{r print-formula-reanalysis}
print(ps_formula_reanalysis)

# Use the reanalysis formula that includes dr_severity
ps_formula <- ps_formula_reanalysis
log_message("Using reanalysis formula with dr_severity for propensity score estimation.")
```

## Run Twang Propensity Score Models

Following the approach in twang-analysis.Rmd, we'll use GBM via twang package to estimate propensity scores.

### Model Parameters
- **Method:** Twang GBM
- **n.trees:** Set based on analysis mode (1000 for test, 3000 for final)
- **interaction.depth:** 3
- **shrinkage:** 0.01
- **estimand:** ATT
- **stop.method:** es.mean (effect size only)

```{r ps-models}
# Initialize logging
log_file <- init_log("propensity_scores")
log_message(paste0("Analysis mode: ", analysis_mode))
log_message(paste0("Number of imputations to process: ", n_imputations))
log_message(paste0("Cache directory: ", cache_dir))
log_message(paste0("Use cache: ", use_cache))

# Function to process a single imputation
process_imputation <- function(i, imp_datasets, formula, trees, cache_path, use_caching, worker_id, log_dir, cache_max_age_days) {
  # Recreate variables inside function to avoid scoping issues
  library(twang)
  
  # Create worker-specific log file
  worker_log_file <- file.path(log_dir, paste0("propensity_scores_worker_", worker_id, ".log"))
  
  # Log start
  start_msg <- paste0("[Worker ", worker_id, "] Imputation ", i, " started")
  cat(paste0(Sys.time(), " - ", start_msg, "\n"), file = worker_log_file, append = TRUE)
  
  # Check for cached result (using consistent naming without underscore)
  cache_file <- file.path(cache_path, paste0("twang_gbm_imp", i, ".rds"))
  
  # Determine if we should use cached file
  should_use_cache <- FALSE
  cache_status <- "not found"
  
  if (use_caching && file.exists(cache_file)) {
    # Check age of cache file
    cache_age_days <- as.numeric(difftime(Sys.time(), file.info(cache_file)$mtime, units = "days"))
    
    if (cache_age_days <= cache_max_age_days) {
      should_use_cache <- TRUE
      cache_status <- paste0("exists, age ", round(cache_age_days, 1), " days (< ", cache_max_age_days, " days) - using cache")
    } else {
      cache_status <- paste0("exists but stale, age ", round(cache_age_days, 1), " days (> ", cache_max_age_days, " days) - recomputing")
    }
  } else if (use_caching) {
    cache_status <- "not found - computing"
  } else {
    cache_status <- "cache disabled - computing"
  }
  
  # Log cache status
  cache_msg <- paste0("[Worker ", worker_id, "] Imputation ", i, " cache: ", cache_status)
  cat(paste0(Sys.time(), " - ", cache_msg, "\n"), file = worker_log_file, append = TRUE)
  
  if (should_use_cache) {
    cached_result <- readRDS(cache_file)
    weights <- cached_result$weights
    
    return(list(
      imputation = i,
      worker_id = worker_id,
      result = cached_result,
      trees_used = cached_result$n_trees_used,
      weight_min = round(min(weights), 3),
      weight_max = round(max(weights), 3),
      cached = TRUE,
      time_mins = 0,
      log_file = worker_log_file
    ))
  } else {
    imp_start <- Sys.time()
    
    # Get the data for this imputation
    imp_data <- as.data.frame(imp_datasets[[i]])
    
    # Ensure ever_lapse_binary is properly formatted as numeric 0/1
    if ("ever_lapse_binary" %in% names(imp_data)) {
      if (is.factor(imp_data$ever_lapse_binary)) {
        # Convert factor to numeric via character to avoid level issues
        imp_data$ever_lapse_binary <- as.numeric(as.character(imp_data$ever_lapse_binary))
      } else if (is.character(imp_data$ever_lapse_binary)) {
        imp_data$ever_lapse_binary <- as.numeric(imp_data$ever_lapse_binary)
      } else if (is.logical(imp_data$ever_lapse_binary)) {
        imp_data$ever_lapse_binary <- as.numeric(imp_data$ever_lapse_binary)
      }
      
      # Ensure values are 0 or 1
      if (any(!imp_data$ever_lapse_binary %in% c(0, 1, NA))) {
        stop(paste("ever_lapse_binary variable contains values other than 0, 1, or NA in imputation", i))
      }
    } else {
      stop(paste("ever_lapse_binary variable not found in imputation", i))
    }
    
    # Run twang ps model
    twang_model <- ps(
      formula = formula,
      data = imp_data,
      n.trees = trees,
      interaction.depth = 3,
      shrinkage = 0.01,
      estimand = "ATT",
      stop.method = c("es.mean"),
      verbose = FALSE
    )
    
    # Extract weights for ATT using es.mean
    weights <- get.weights(twang_model, stop.method = "es.mean")
    
    # Store results
    result <- list(
      model = twang_model,
      weights = weights,
      n_trees_used = twang_model$desc$es.mean$n.trees
    )
    
    # Save to cache
    saveRDS(result, cache_file)
    
    imp_time <- round(as.numeric(difftime(Sys.time(), imp_start, units = "mins")), 2)
    
    # Log completion
    complete_msg <- paste0("[Worker ", worker_id, "] Imputation ", i, " completed in ", imp_time, " minutes")
    cat(paste0(Sys.time(), " - ", complete_msg, "\n"), file = worker_log_file, append = TRUE)
    
    return(list(
      imputation = i,
      worker_id = worker_id,
      result = result,
      trees_used = result$n_trees_used,
      weight_min = round(min(weights), 3),
      weight_max = round(max(weights), 3),
      cached = FALSE,
      time_mins = imp_time,
      log_file = worker_log_file
    ))
  }
}

# Create log directory for worker logs
log_dir <- dirname(log_file)

# Run sequential processing (parallel clustering removed due to stability issues)
log_message(paste0("Starting sequential processing of ", n_imputations, " imputations"))
sequential_start <- Sys.time()

# Process each imputation sequentially
sequential_results <- lapply(
  1:n_imputations,
  function(i) {
    worker_id <- 1  # Single worker for sequential processing
    process_imputation(i, imputed_datasets, ps_formula, n_trees, cache_dir, use_cache, worker_id, log_dir, cache_max_age_days)
  }
)

sequential_time <- round(as.numeric(difftime(Sys.time(), sequential_start, units = "mins")), 2)
log_message(paste0("Sequential processing completed in ", sequential_time, " minutes"))

# Extract results and build progress dataframe
twang_results <- list()
progress_df <- data.frame(
  Imputation = integer(),
  Worker_ID = integer(),
  Trees_Used = integer(),
  Weight_Min = numeric(),
  Weight_Max = numeric(),
  Cached = logical(),
  Time_Minutes = numeric()
)

for (res in sequential_results) {
  twang_results[[res$imputation]] <- res$result
  
  progress_df <- rbind(progress_df, data.frame(
    Imputation = res$imputation,
    Worker_ID = res$worker_id,
    Trees_Used = res$trees_used,
    Weight_Min = res$weight_min,
    Weight_Max = res$weight_max,
    Cached = res$cached,
    Time_Minutes = res$time_mins
  ))
  
  # Log individual result summary in main log (details in worker log)
  if (res$cached) {
    log_message(paste0("Imputation ", res$imputation, ": Used cached result"))
  } else {
    log_message(paste0("Imputation ", res$imputation, ": Computed in ", res$time_mins, " minutes"))
  }
}

# Read worker log (single sequential worker)
log_message("=== Processing Log Summary ===")
worker_log_file <- file.path(log_dir, "propensity_scores_worker_1.log")
if (file.exists(worker_log_file)) {
  worker_logs <- readLines(worker_log_file)
  log_message(paste0("Sequential processing: ", 
                    sum(grepl("started", worker_logs)), " imputations started, ",
                    sum(grepl("completed", worker_logs)), " completed"))
  # Optionally show first and last entries
  if (length(worker_logs) > 0) {
    log_message(paste0("  First: ", worker_logs[1]))
    log_message(paste0("  Last:  ", worker_logs[length(worker_logs)]))
  }
}
log_message("==========================")

# Cache usage summary
n_cached <- sum(progress_df$Cached)
n_computed <- sum(!progress_df$Cached)
cache_time_saved <- round(n_cached * mean(progress_df$Time_Minutes[!progress_df$Cached], na.rm = TRUE), 1)

log_message("=== Cache Summary ===")
log_message(paste0("Total imputations: ", n_imputations))
log_message(paste0("Used cache: ", n_cached, " (", round(100 * n_cached / n_imputations, 1), "%)"))
log_message(paste0("Newly computed: ", n_computed, " (", round(100 * n_computed / n_imputations, 1), "%)"))
if (n_computed > 0 && n_cached > 0) {
  log_message(paste0("Estimated time saved: ~", cache_time_saved, " minutes"))
}
log_message("=====================")

# Sort progress_df by imputation number
progress_df <- progress_df[order(progress_df$Imputation), ]

# Display progress summary with caching and timing information
kable(progress_df, 
      caption = paste0("Twang Model Fitting Progress (Total time: ", sequential_time, " minutes - sequential processing)"),
      col.names = c("Imputation", "Worker", "Trees Used", "Min Weight", "Max Weight", "From Cache", "Time (min)"))
```

## Balance Assessment

```{r balance-assessment}
# Extract balance summaries from twang models
balance_summaries <- list()

for (i in 1:n_imputations) {
  # Get the twang model
  twang_model <- twang_results[[i]]$model
  
  weights <- twang_results[[i]]$weights
  treated <- imputed_datasets[[i]]$ever_lapse_binary == 1
  
  balance_summaries[[i]] <- data.frame(
    Imputation = i,
    N_Treated = sum(treated),
    N_Control = sum(!treated),
    Mean_Weight_Treated = mean(weights[treated]),
    Mean_Weight_Control = mean(weights[!treated]),
    Max_Weight = max(weights)
  )
  
  if (i <= max_imputations_for_diagnostics) {
    # Balance summary will be shown in diagnostics section
  }
}

balance_summary_all <- do.call(rbind, balance_summaries)

```

### Weight Summary Across All Imputations

```{r weight-summary-table}
kable(balance_summary_all, 
      caption = "Weight distribution by imputation",
      digits = 3)

# Calculate average across imputations
avg_weights <- data.frame(
  Avg_N_Treated = mean(balance_summary_all$N_Treated),
  Avg_N_Control = mean(balance_summary_all$N_Control),
  Avg_Weight_Treated = mean(balance_summary_all$Mean_Weight_Treated),
  Avg_Weight_Control = mean(balance_summary_all$Mean_Weight_Control),
  Max_Weight_Overall = max(balance_summary_all$Max_Weight)
)

```

### Average Weights Across Imputations

```{r avg-weights-table}
kable(avg_weights,
      caption = "Average weight statistics across all imputations",
      digits = 3)

```

### Weight Stability Check

```{r weight-stability-check}
if (avg_weights$Max_Weight_Overall > 10) {
  print("Warning: Maximum weight > 10, indicating potential instability")
} else if (avg_weights$Max_Weight_Overall > 5) {
  print("Note: Maximum weight > 5, monitor for stability")
} else {
  print("Weights appear stable (max < 5)")
}
```

## Diagnostics for Sample of Imputations

Showing diagnostic plots for the first **`r min(max_imputations_for_diagnostics, n_imputations)`** imputations.

```{r diagnostics-plots}
#| fig-height: 8
#| fig-width: 10

n_to_show <- min(max_imputations_for_diagnostics, n_imputations)
imps_to_show <- 1:n_to_show
```

```{r diagnostics-loop}
#| results: "asis"

# Generate diagnostic plots for selected imputations
for (i in imps_to_show) {
  log_message(paste0("Processing diagnostics for Imputation ", i))
  cat("\n### Imputation", i, "\n\n")
  
  twang_model <- twang_results[[i]]$model
  weights <- twang_results[[i]]$weights
  
  # Create dataset with weights for cobalt
  data_with_weights <- imputed_datasets[[i]]
  data_with_weights$weights <- weights
  
  cat("#### Convergence (Iteration Plot)\n\n")
  # Plot 1: Shows how balance metrics change across iterations
  print(plot(twang_model, plots = 1))
  cat("\n\n")
  
  cat("#### Weight Distribution (Boxplot)\n\n")
  # Plot 2: Shows distribution of weights
  print(plot(twang_model, plots = 2))
  cat("\n\n")
  
  cat("#### Covariate Balance (Standardized Mean Differences)\n\n")
  # Plot 3: Shows balance for each covariate
  print(plot(twang_model, plots = 3))
  cat("\n\n")
  
  cat("#### Love Plot\n\n")
  # Use cobalt for better visualization
  love_plot <- love.plot(
    bal.tab(ps_formula, 
            data = data_with_weights,
            weights = "weights",
            estimand = "ATT",
            s.d.denom = "treated"),
    thresholds = c(m = 0.1),
    title = paste("Covariate Balance - Imputation", i)
  )
  print(love_plot)
  cat("\n\n")
  
  cat("#### Weight Histogram\n\n")
  # Plot 6: Histogram of weight proportions
  print(plot(twang_model, plots = 6))
  cat("\n\n")
  
  cat("#### Balance Summary Table\n\n")
  
  # Get balance statistics from the twang model
  # The bal.table function gives us the detailed balance statistics
  tryCatch({
    bal_table <- bal.table(twang_model, digits = 4)
    
    # Extract the summary statistics for the optimal stopping point
    # twang stores these in the desc slot
    es_mean_stats <- twang_model$desc$es.mean
    
    # Safely extract values with error checking
    safe_round <- function(x, digits = 4) {
      if (is.null(x) || !is.numeric(x)) return(NA)
      return(round(x, digits))
    }
    
    # Create a clean summary table
    summary_df <- data.frame(
      Metric = c("N Treated", "N Control", 
                 "ESS Treated", "ESS Control",
                 "Max ES", "Mean ES", "Max KS",
                 "Iterations"),
      Unweighted = c(
        es_mean_stats$n.treat,
        es_mean_stats$n.ctrl,
        es_mean_stats$n.treat,  # ESS = n for unweighted
        es_mean_stats$n.ctrl,    # ESS = n for unweighted
        safe_round(bal_table$unw$max.es, 4),
        safe_round(bal_table$unw$mean.es, 4),
        safe_round(bal_table$unw$max.ks, 4),
        NA
      ),
      Weighted = c(
        es_mean_stats$n.treat,
        es_mean_stats$n.ctrl,
        safe_round(es_mean_stats$ess.treat, 2),
        safe_round(es_mean_stats$ess.ctrl, 2),
        safe_round(bal_table$es.mean$max.es, 4),
        safe_round(bal_table$es.mean$mean.es, 4),
        safe_round(bal_table$es.mean$max.ks, 4),
        es_mean_stats$n.trees
      )
    )
    
    kable(summary_df, 
          caption = paste("Balance Summary for Imputation", i),
          align = c('l', 'r', 'r'))
    
  }, error = function(e) {
    # Fallback to simpler summary if bal.table fails
    log_message("Note: Detailed balance table unavailable. Showing basic summary.")
    
    # Use the summary method which is more reliable
    sum_output <- summary(twang_model)
    
    # Try to extract key metrics from the summary
    if (!is.null(sum_output)) {
      print(sum_output)
    } else {
      log_message("Balance statistics not available for this imputation.")
    }
  })
  
  # Spacing
}
```

## Propensity Score Distribution

```{r ps-distribution}
#| fig-height: 6
#| fig-width: 10

# Create weight distribution summary
weight_dist_summary <- data.frame(
  Imputation = integer(),
  Control_Mean = numeric(),
  Control_Max = numeric(),
  Treated_Mean = numeric()
)

# Plot PS distribution for first few imputations
for (i in imps_to_show) {
  
  twang_model <- twang_results[[i]]$model
  weights <- twang_results[[i]]$weights
  treated <- imputed_datasets[[i]]$ever_lapse_binary == 1
  
  par(mfrow = c(1, 2))
  
  # Control group weights
  hist(weights[!treated], 
       main = paste("Weights: Control - Imputation", i),
       xlab = "Weight",
       col = "lightblue",
       breaks = 20)
  
  # Treated group weights (should all be 1 for ATT)
  hist(weights[treated],
       main = paste("Weights: Treated - Imputation", i),
       xlab = "Weight", 
       col = "lightcoral",
       breaks = 20)
  
  par(mfrow = c(1, 1))
  
  # Store weight summary
  weight_dist_summary <- rbind(weight_dist_summary, data.frame(
    Imputation = i,
    Control_Mean = round(mean(weights[!treated]), 3),
    Control_Max = round(max(weights[!treated]), 3),
    Treated_Mean = round(mean(weights[treated]), 3)
  ))
}

# Display weight distribution summary
kable(weight_dist_summary, 
      caption = "Weight Distribution by Imputation",
      col.names = c("Imputation", "Control Mean", "Control Max", "Treated Mean (should be 1.0 for ATT)"))
```


## Save Propensity Score Results

```{r save-results}
# Extract weights from all twang models for easy access in later steps
twang_weights <- list()
for (i in 1:n_imputations) {
  twang_weights[[i]] <- twang_results[[i]]$weights
}

ps_output <- list(
  twang_results = twang_results,
  weight_summary = balance_summary_all,
  avg_weights = avg_weights,
  formula_used = ps_formula,
  n_imputations = n_imputations,
  method = "twang_gbm",
  n_trees = n_trees,
  analysis_mode = analysis_mode
)

saveRDS(ps_output, file.path(reanalysis_data_dir, "ps_results_twang.rds"))
saveRDS(twang_weights, file.path(reanalysis_data_dir, "twang_weights.rds"))

```

**Propensity score results saved to:** `r file.path(reanalysis_data_dir, "ps_results_twang.rds")`
**Weights saved to:** `r file.path(reanalysis_data_dir, "twang_weights.rds")`
**Cache directory:** `r cache_dir`

```{r cache-summary}
# Check cache status
cache_files <- list.files(cache_dir, pattern = "twang_gbm_imp.*\\.rds", full.names = FALSE)
n_cached <- length(cache_files)

cache_status_msg <- paste0("Cache status: ", n_cached, " of ", n_imputations, " imputations cached")
log_message(cache_status_msg)

if (n_cached > 0) {
  log_message("To force re-computation, set use_cache <- FALSE in the setup chunk")
}

# Finalize logging
final_log <- finalize_log(success = TRUE)
```

**Log file location:** `r final_log`

To monitor the log during execution:
```bash
tail -f `r final_log`
```

## Average Largest Absolute Standardized Mean Difference

```{r avg-largest-smd}
# Extract the largest absolute SMD for each imputation
largest_smds <- numeric(n_imputations)

for (i in 1:n_imputations) {
  twang_model <- twang_results[[i]]$model
  
  # Get balance table which contains standardized mean differences
  tryCatch({
    bal_table <- bal.table(twang_model)
    
    # Extract the weighted (after matching) standardized effect sizes
    # The bal.table contains effect sizes for each covariate
    if (!is.null(bal_table$es.mean) && !is.null(bal_table$es.mean$max.es)) {
      largest_smds[i] <- bal_table$es.mean$max.es
    } else {
      # Fallback: use the summary method
      model_summary <- summary(twang_model)
      if (!is.null(model_summary) && "max.es" %in% names(model_summary)) {
        largest_smds[i] <- model_summary$max.es
      } else {
        largest_smds[i] <- NA
      }
    }
  }, error = function(e) {
    # If balance table fails, set to NA
    largest_smds[i] <- NA
  })
}

# Calculate average of largest SMDs across imputations
avg_largest_smd <- mean(largest_smds, na.rm = TRUE)
n_valid_smds <- sum(!is.na(largest_smds))

# Create summary table
smd_summary <- data.frame(
  Imputation = 1:n_imputations,
  Largest_Absolute_SMD = round(largest_smds, 4)
)

kable(smd_summary, 
      caption = "Largest absolute standardized mean difference by imputation",
      col.names = c("Imputation", "Largest |SMD|"))

# Display average
cat("\n**Average of largest absolute SMDs across imputations:**", round(avg_largest_smd, 4), "\n")
cat("**Number of valid SMD values:**", n_valid_smds, "out of", n_imputations, "\n")

if (avg_largest_smd < 0.1) {
  cat("**Interpretation:** Excellent balance (avg largest |SMD| < 0.1)\n")
} else if (avg_largest_smd < 0.25) {
  cat("**Interpretation:** Good balance (avg largest |SMD| < 0.25)\n") 
} else {
  cat("**Interpretation:** Poor balance (avg largest |SMD| ≥ 0.25) - consider alternative methods\n")
}
```

## Summary

### Analysis Configuration

**Method:** Generalized Boosted Models (GBM) via twang  
**Estimand:** ATT (Average Treatment Effect on the Treated)  
**Stop Method:** es.mean (effect size minimization)  

```{r summary-config}
config_summary <- data.frame(
  Parameter = c("Analysis Mode", "Imputations Processed", "Trees per Model"),
  Value = c(analysis_mode, n_imputations, n_trees)
)

kable(config_summary, caption = "Run Configuration")
```

### Formula Components

The enhanced propensity score formula includes:
- DR severity (person_dr)
- Treatment type variables
- All standard covariates from original analysis

```{r summary-formula}
print(ps_formula)
```

### Weight Statistics Summary

```{r summary-weights}
weight_stats <- data.frame(
  Statistic = c("Mean Weight (Treated)", "Mean Weight (Control)", "Maximum Weight"),
  Value = c(round(avg_weights$Avg_Weight_Treated, 3),
           round(avg_weights$Avg_Weight_Control, 3),
           round(avg_weights$Max_Weight_Overall, 2))
)

kable(weight_stats, caption = "Weight Statistics (Averaged Across Imputations)")
```
